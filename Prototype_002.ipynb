{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRED41Z/aXZVptDo3tOIXY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CyberMetrics/Prototypes/blob/main/Prototype_002.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJKSDY0sRxRd",
        "outputId": "f7de6b6c-29fe-4aaa-ea03-d266a302175f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n"
      ],
      "metadata": {
        "id": "CB3HatZgR1oj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/My Drive/Capstone Mark-01/wazuh_sample(csv).csv'\n",
        "\n",
        "df_full = pd.read_csv(file_path)\n",
        "print(\"File loaded successfully — shape:\", df_full.shape)\n",
        "print(\"Columns:\", list(df_full.columns))\n",
        "print(df_full.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng_t-Kn0R6-f",
        "outputId": "ecf9e918-297b-4000-dcc1-b19afe4c08d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File loaded successfully — shape: (1000, 14)\n",
            "Columns: ['timestamp', 'rule_id', 'rule_level', 'agent_id', 'agent_name', 'srcip', 'dstip', 'event_type', 'action', 'status', 'severity', 'group', 'message', 'system']\n",
            "              timestamp  rule_id  rule_level  agent_id    agent_name  \\\n",
            "0  2025-10-13T18:00:00Z     1005           7         1  web-server-1   \n",
            "1  2025-10-13T18:00:15Z     1031           2         4   mail-server   \n",
            "2  2025-10-13T18:00:30Z     1036           3         3    firewall-1   \n",
            "3  2025-10-13T18:00:45Z     1048           1         1  web-server-1   \n",
            "4  2025-10-13T18:01:00Z     1021           3         5  proxy-server   \n",
            "\n",
            "             srcip         dstip              event_type   action   status  \\\n",
            "0  192.168.216.215    10.0.81.30               port_scan   detect  success   \n",
            "1   192.168.91.137    10.0.3.165          file_integrity     scan  success   \n",
            "2  192.168.228.116   10.0.34.134  authentication_failure  execute  success   \n",
            "3  192.168.213.103  10.0.184.248       data_exfiltration  execute  blocked   \n",
            "4    192.168.53.66   10.0.119.29       data_exfiltration     scan  success   \n",
            "\n",
            "   severity           group  \\\n",
            "0      high  authentication   \n",
            "1  critical            user   \n",
            "2  critical          system   \n",
            "3    medium            user   \n",
            "4    medium  authentication   \n",
            "\n",
            "                                             message   system  \n",
            "0             Port scan detected with status success    macos  \n",
            "1        File integrity detected with status success    macos  \n",
            "2  Authentication failure detected with status su...    linux  \n",
            "3     Data exfiltration detected with status blocked  windows  \n",
            "4     Data exfiltration detected with status success    macos  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SimpleLogisticRegression:\n",
        "    \"\"\"A minimal implementation of Logistic Regression using Gradient Descent.\"\"\"\n",
        "    def __init__(self, lr=0.01, epochs=1000):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.w = None\n",
        "        self.b = 0\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        z_clip = np.clip(z, -500, 500)\n",
        "        return 1 / (1 + np.exp(-z_clip))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.array(X, dtype=float)\n",
        "        y = np.array(y, dtype=float)\n",
        "        n, d = X.shape\n",
        "        self.w = np.zeros(d)\n",
        "        self.b = 0\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            z = X.dot(self.w) + self.b\n",
        "            pred = self._sigmoid(z)\n",
        "            grad_w = (1/n) * X.T.dot(pred - y)\n",
        "            grad_b = (1/n) * np.sum(pred - y)\n",
        "            self.w -= self.lr * grad_w\n",
        "            self.b -= self.lr * grad_b\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        X = np.array(X, dtype=float)\n",
        "        z = X.dot(self.w) + self.b\n",
        "        return self._sigmoid(z)\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        return (self.predict_proba(X) >= threshold).astype(int)\n"
      ],
      "metadata": {
        "id": "Q3p1M8C9TcHZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WazuhEventAnalyzer:\n",
        "    \"\"\"\n",
        "    A lightweight SIEM-style analyzer for Wazuh-like logs.\n",
        "    Performs preprocessing, anomaly detection, ML classification, and sequence analysis.\n",
        "    \"\"\"\n",
        "    def __init__(self, lr=0.01, epochs=1000):\n",
        "        self.classifier = SimpleLogisticRegression(lr=lr, epochs=epochs)\n",
        "        self.label_encoders = {}\n",
        "        self.is_trained = False\n",
        "        # Adjusted to fit your dataset\n",
        "        self.feature_cols = [\n",
        "            'rule_level_scaled',\n",
        "            'rule_id_scaled',\n",
        "            'severity_encoded',\n",
        "            'status_encoded',\n",
        "            'Anomaly_Flag'\n",
        "        ]\n",
        "\n",
        "    def _preprocess(self, df):\n",
        "        df_proc = df.copy()\n",
        "\n",
        "        # 1️ Binary target based on severity\n",
        "        df_proc['is_critical_event'] = df_proc['severity'].apply(\n",
        "            lambda x: 1 if str(x).lower() in ['high', 'critical'] else 0\n",
        "        )\n",
        "\n",
        "        # 2️ Label encode categorical columns safely\n",
        "        categorical_cols = ['agent_name', 'status', 'severity', 'action', 'event_type', 'system']\n",
        "        for col in categorical_cols:\n",
        "            if col in df_proc.columns:\n",
        "                if col not in self.label_encoders:\n",
        "                    le = LabelEncoder()\n",
        "                    df_proc[f'{col}_encoded'] = le.fit_transform(df_proc[col].astype(str))\n",
        "                    self.label_encoders[col] = le\n",
        "                else:\n",
        "                    le = self.label_encoders[col]\n",
        "                    df_proc[f'{col}_encoded'] = df_proc[col].apply(\n",
        "                        lambda x: le.transform([x])[0] if x in le.classes_ else -1\n",
        "                    )\n",
        "\n",
        "        # 3️ Scale numeric columns (rule_id, rule_level)\n",
        "        numeric_cols_to_scale = ['rule_id', 'rule_level']\n",
        "        for col in numeric_cols_to_scale:\n",
        "            if col in df_proc.columns:\n",
        "                data = df_proc[col].values.reshape(-1, 1)\n",
        "                if col not in self.label_encoders:\n",
        "                    scaler = StandardScaler()\n",
        "                    df_proc[f'{col}_scaled'] = scaler.fit_transform(data)\n",
        "                    self.label_encoders[col] = scaler\n",
        "                else:\n",
        "                    scaler = self.label_encoders[col]\n",
        "                    df_proc[f'{col}_scaled'] = scaler.transform(data)\n",
        "\n",
        "        # 4️ Time delta inverse (event frequency indicator)\n",
        "        df_proc['timestamp'] = pd.to_datetime(df_proc['timestamp'])\n",
        "        df_proc = df_proc.sort_values('timestamp').reset_index(drop=True)\n",
        "        time_diff = df_proc['timestamp'].diff().dt.total_seconds().fillna(0)\n",
        "        df_proc['time_delta_inv'] = 1 / (time_diff + 1e-6)\n",
        "\n",
        "        return df_proc\n",
        "\n",
        "    def fit(self, df):\n",
        "            df_proc = self._preprocess(df)\n",
        "            # Anomaly Detection (rare rule IDs)\n",
        "            event_counts = df_proc[\"rule_id\"].value_counts()\n",
        "            df_proc['Anomaly_Flag'] = df_proc[\"rule_id\"].apply(\n",
        "                lambda x: 1 if event_counts[x] == 1 else 0\n",
        "            )\n",
        "\n",
        "            X = df_proc[self.feature_cols]\n",
        "            y = df_proc['is_critical_event']\n",
        "\n",
        "            self.classifier.fit(X.values, y.values)\n",
        "            self.is_trained = True\n",
        "            print(f\"Classifier trained with {len(X)} samples.\")\n",
        "\n",
        "    def analyze(self, df):\n",
        "            if df.empty:\n",
        "                return pd.DataFrame()\n",
        "\n",
        "            df_proc = self._preprocess(df)\n",
        "            event_counts = df_proc[\"rule_id\"].value_counts()\n",
        "            df_proc['Anomaly_Flag'] = df_proc[\"rule_id\"].apply(\n",
        "                lambda x: 1 if event_counts[x] == 1 else 0\n",
        "            )\n",
        "\n",
        "            # ML-based classification\n",
        "            if not self.is_trained:\n",
        "                df_proc['ML_Proba'] = 0.5\n",
        "                df_proc['ML_Prediction'] = -1\n",
        "            else:\n",
        "                X_test = df_proc[self.feature_cols]\n",
        "                df_proc['ML_Proba'] = self.classifier.predict_proba(X_test.values)\n",
        "                df_proc['ML_Prediction'] = (df_proc['ML_Proba'] >= 0.5).astype(int)\n",
        "\n",
        "            # Simple sequence detection (fast bursts from same srcip)\n",
        "            seq_alert_check = df_proc.groupby('srcip')['time_delta_inv'].transform(\n",
        "                lambda x: x.rolling(window=3, min_periods=1).mean()\n",
        "            )\n",
        "            df_proc['Seq_Alert'] = (df_proc['time_delta_inv'] > seq_alert_check.shift(1).fillna(0) * 2).astype(int)\n",
        "\n",
        "            # Weighted final score\n",
        "            df_proc['Final_Score'] = (\n",
        "                df_proc['ML_Proba'] * 0.5 +\n",
        "                df_proc['Anomaly_Flag'] * 0.3 +\n",
        "                df_proc['Seq_Alert'] * 0.2\n",
        "            )\n",
        "            df_proc['FinalAlert'] = (df_proc['Final_Score'] > 0.7).astype(int)\n",
        "\n",
        "            return df_proc[['timestamp', 'rule_id', 'severity', 'Anomaly_Flag', 'ML_Proba',\n",
        "                            'Seq_Alert', 'Final_Score', 'FinalAlert']]"
      ],
      "metadata": {
        "id": "KgG8dTLATnmK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_full.head(200).copy()\n",
        "df_batch = df_full.tail(df_full.shape[0] - 200).copy()\n",
        "\n",
        "event_analyzer = WazuhEventAnalyzer(lr=0.05, epochs=2000)\n",
        "print(\"\\n--- Training the WazuhEventAnalyzer ---\")\n",
        "event_analyzer.fit(df_train)\n",
        "\n",
        "print(\"\\n--- Sequential Event Analysis on New Batch ---\")\n",
        "results = event_analyzer.analyze(df_batch)\n",
        "print(results.head(20).to_markdown(index=False, floatfmt=\".2f\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r1f_aEIT-vk",
        "outputId": "b0315f07-1975-4003-a65c-cb806ec05c6e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training the WazuhEventAnalyzer ---\n",
            "Classifier trained with 200 samples.\n",
            "\n",
            "--- Sequential Event Analysis on New Batch ---\n",
            "| timestamp                 |   rule_id | severity   |   Anomaly_Flag |   ML_Proba |   Seq_Alert |   Final_Score |   FinalAlert |\n",
            "|:--------------------------|----------:|:-----------|---------------:|-----------:|------------:|--------------:|-------------:|\n",
            "| 2025-10-13 18:50:00+00:00 |      1013 | critical   |              0 |       1.00 |           1 |          0.70 |            0 |\n",
            "| 2025-10-13 18:50:15+00:00 |      1012 | high       |              0 |       0.77 |           0 |          0.38 |            0 |\n",
            "| 2025-10-13 18:50:30+00:00 |      1005 | high       |              0 |       0.90 |           0 |          0.45 |            0 |\n",
            "| 2025-10-13 18:50:45+00:00 |      1010 | medium     |              0 |       0.00 |           0 |          0.00 |            0 |\n",
            "| 2025-10-13 18:51:00+00:00 |      1012 | medium     |              0 |       0.00 |           0 |          0.00 |            0 |\n",
            "| 2025-10-13 18:51:15+00:00 |      1036 | medium     |              0 |       0.01 |           0 |          0.00 |            0 |\n",
            "| 2025-10-13 18:51:30+00:00 |      1047 | high       |              0 |       0.87 |           0 |          0.44 |            0 |\n",
            "| 2025-10-13 18:51:45+00:00 |      1023 | critical   |              0 |       0.99 |           0 |          0.49 |            0 |\n",
            "| 2025-10-13 18:52:00+00:00 |      1010 | low        |              0 |       0.15 |           0 |          0.07 |            0 |\n",
            "| 2025-10-13 18:52:15+00:00 |      1004 | critical   |              0 |       1.00 |           0 |          0.50 |            0 |\n",
            "| 2025-10-13 18:52:30+00:00 |      1006 | high       |              0 |       0.83 |           0 |          0.41 |            0 |\n",
            "| 2025-10-13 18:52:45+00:00 |      1031 | medium     |              0 |       0.00 |           0 |          0.00 |            0 |\n",
            "| 2025-10-13 18:53:00+00:00 |      1040 | critical   |              0 |       0.99 |           0 |          0.50 |            0 |\n",
            "| 2025-10-13 18:53:15+00:00 |      1010 | critical   |              0 |       1.00 |           0 |          0.50 |            0 |\n",
            "| 2025-10-13 18:53:30+00:00 |      1049 | high       |              0 |       0.68 |           0 |          0.34 |            0 |\n",
            "| 2025-10-13 18:53:45+00:00 |      1039 | high       |              0 |       0.68 |           0 |          0.34 |            0 |\n",
            "| 2025-10-13 18:54:00+00:00 |      1031 | low        |              0 |       0.19 |           0 |          0.09 |            0 |\n",
            "| 2025-10-13 18:54:15+00:00 |      1030 | high       |              0 |       0.88 |           0 |          0.44 |            0 |\n",
            "| 2025-10-13 18:54:30+00:00 |      1005 | critical   |              0 |       0.99 |           0 |          0.50 |            0 |\n",
            "| 2025-10-13 18:54:45+00:00 |      1021 | critical   |              0 |       1.00 |           0 |          0.50 |            0 |\n"
          ]
        }
      ]
    }
  ]
}